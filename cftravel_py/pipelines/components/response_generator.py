"""
Response Generator Component for ASIA.fr Agent
Handles text generation and response formatting
"""

import json
from typing import Dict, Any, List
from ..core import PipelineComponent, PipelineContext, PipelineState
from services.llm_service import LLMService
from services.memory_service import MemoryService

class ResponseGeneratorComponent(PipelineComponent):
    """Generates intelligent responses based on pipeline context"""
    
    def __init__(self, llm_service: LLMService, memory_service: MemoryService):
        super().__init__("ResponseGenerator", priority=70)
        self.llm_service = llm_service
        self.memory_service = memory_service
    
    def is_required(self, context: PipelineContext) -> bool:
        """Always required to generate a response"""
        return True
    
    async def process(self, context: PipelineContext) -> PipelineContext:
        """Generate appropriate response based on context"""
        try:
            # Determine response type and generate accordingly
            response_type = context.get_metadata('response_type', 'question')
            intent = context.get_metadata('intent', 'general')
            
            if intent == 'greeting':
                # Handle greetings, thanks, and goodbyes naturally
                response = await self._generate_natural_response(context, "greeting")
            elif intent == 'preference_complete':
                # Generate text-based preference summary
                response = await self._generate_preference_summary_response(context)
            elif intent == 'confirmation':
                # User confirmed preferences - ensure offers are generated first
                context.add_metadata('should_show_offers', True)
                # The recommendation engine will be called by the pipeline to generate offers
                response = await self._generate_natural_response(context, "confirmation")
            elif intent == 'modification':
                # User wants to modify preferences - help them naturally
                response = await self._generate_natural_response(context, "modification")
            elif intent == 'suggestion_request':
                # User wants AI suggestions - provide helpful recommendations
                response = await self._generate_natural_response(context, "suggestion")
            elif intent == 'general':
                # General conversation - respond naturally
                response = await self._generate_natural_response(context, "conversation")
            elif context.get_metadata('should_show_offers', False):
                # Check if offers are available (generated by recommendation engine)
                offers = context.get_metadata('offers', [])
                if offers:
                    # Generate offer presentation response
                    response = await self._generate_offer_response(context)
                else:
                    # Offers not ready yet, generate confirmation message
                    response = await self._generate_confirmation_response(context)
            else:
                # Generate general conversation response
                response = await self._generate_general_response(context)
            
            # Store response in context
            context.add_metadata('generated_response', response)
            context.add_metadata('response_text', response.get('text', ''))
            
            return context
            
        except Exception as e:
            self.log_error(context, e)
            # Fallback response
            context.add_metadata('generated_response', self._fallback_response())
            return context
    
    async def _generate_offer_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate response when showing offers"""
        offers = context.get_metadata('offers', [])
        
        if not offers:
                    return {
            'text': "Je n'ai pas trouvé d'offres correspondant exactement à vos critères. Pouvez-vous préciser vos préférences ?",
            'type': 'no_offers',
            'offers': []
        }
    
    async def _generate_natural_response(self, context: PipelineContext, response_type: str) -> Dict[str, Any]:
        """Generate natural, intelligent responses using LLM"""
        try:
            # Build context-aware prompt
            prompt = self._build_natural_response_prompt(context, response_type)
            
            # Get LLM response
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            
            return {
                'text': response.strip(),
                'type': response_type,
                'should_show_offers': response_type == 'confirmation'
            }
            
        except Exception as e:
            self.logger.error(f"❌ Natural response generation failed: {e}")
            # Fallback response
            return {
                'text': "Je suis désolé, je n'ai pas bien compris. Pouvez-vous reformuler ? 😊",
                'type': response_type,
                'should_show_offers': False
            }
    
    def _build_natural_response_prompt(self, context: PipelineContext, response_type: str) -> str:
        """Build prompt for natural response generation"""
        user_input = context.user_input
        preferences = context.user_preferences
        conversation_history = context.conversation_history[-5:] if context.conversation_history else []
        
        # Build context string
        context_str = ""
        if preferences:
            context_str = f"\nPréférences actuelles: {json.dumps(preferences, indent=2, ensure_ascii=False)}"
        
        history_str = ""
        if conversation_history:
            history_str = f"\nHistorique récent: {conversation_history}"
        
        base_prompt = f"""
Tu es ASIA.fr Agent, un spécialiste du voyage en Asie. Tu dois répondre en français de manière naturelle et chaleureuse.

CONTEXTE:{context_str}{history_str}

MESSAGE UTILISATEUR: "{user_input}"

TYPE DE RÉPONSE REQUISE: {response_type}

INSTRUCTIONS:
- Réponds de manière naturelle et conversationnelle
- Sois chaleureux et enthousiaste
- Utilise des emojis appropriés
- Adapte ta réponse au contexte et à l'historique
- Si c'est une modification, aide l'utilisateur à ajuster ses préférences
- Si c'est une suggestion, propose des idées créatives
- Si c'est une confirmation, confirme et prépare pour les offres
- Si c'est une conversation générale, sois utile et engageant

RÉPONSE:
"""
        
        # Add specific instructions based on response type
        if response_type == "greeting":
            base_prompt += "\n- Si c'est un remerciement, réponds poliment\n- Si c'est un au revoir, sois chaleureux\n- Si c'est une salutation, propose ton aide"
        elif response_type == "modification":
            base_prompt += "\n- Aide l'utilisateur à modifier ses préférences\n- Propose des alternatives\n- Sois flexible et compréhensif"
        elif response_type == "suggestion":
            base_prompt += "\n- Propose des destinations intéressantes\n- Suggère des expériences uniques\n- Sois créatif et inspirant"
        elif response_type == "confirmation":
            base_prompt += "\n- Confirme les préférences\n- Prépare pour l'affichage des offres\n- Sois enthousiaste"
        elif response_type == "conversation":
            base_prompt += "\n- Réponds naturellement à la conversation\n- Sois utile et engageant\n- Guide vers la planification de voyage si approprié"
        
        return base_prompt
    
    async def _generate_offer_intro(self, context: PipelineContext, offers: List[Dict]) -> str:
        """Generate personalized introduction for offers"""
        try:
            prompt = f"""
You are ASIA.fr Agent, a French travel specialist. Generate a personalized introduction for the offers you're about to present.

USER PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}
OFFER COUNT: {len(offers)}

Generate a warm, personalized introduction in French that:
1. Acknowledges their preferences
2. Mentions the number of offers found
3. Invites them to explore the offers
4. Is friendly and professional

Keep it concise (2-3 sentences maximum).

RESPOND ONLY WITH THE INTRODUCTION TEXT:
"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"❌ Failed to generate offer intro: {e}")
            return f"Voici {len(offers)} offres qui correspondent parfaitement à vos critères :"
    
    async def _generate_confirmation_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate response for user confirmations"""
        try:
            prompt = f"""
You are ASIA.fr Agent, a friendly and enthusiastic French travel specialist with a warm personality like Layla.ai. The user has confirmed their preferences and you're excited to show them amazing offers!

USER PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}

Generate a warm, enthusiastic confirmation response in French that:
1. 🌟 Acknowledges their confirmation with genuine excitement
2. ✨ Shows enthusiasm about preparing their personalized offers
3. Uses emojis naturally to express your happiness
4. Sounds like you're genuinely excited to help them

Keep it warm and encouraging (1-2 sentences).

RESPOND ONLY WITH THE CONFIRMATION TEXT:
"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            return {
                'text': response.strip(),
                'type': 'confirmation'
            }
            
        except Exception as e:
            self.logger.error(f"❌ Failed to generate confirmation response: {e}")
            return {
                'text': "Parfait ! Je prépare vos offres personnalisées.",
                'type': 'confirmation'
            }
    
    async def _generate_modification_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate response for modification requests"""
        try:
            prompt = f"""
You are ASIA.fr Agent, a friendly and enthusiastic French travel specialist with a warm personality like Layla.ai. The user wants to modify their preferences and you're happy to help them get it perfect!

CURRENT PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}
USER INPUT: {context.user_input}

Generate a warm, helpful response in French that:
1. 🌟 Acknowledges their desire to modify preferences with understanding
2. ✨ Shows enthusiasm about helping them get their perfect trip
3. Asks what they'd like to change with genuine interest
4. Uses emojis naturally to keep it friendly
5. Sounds like you're genuinely happy to help them adjust

Keep it conversational, warm and encouraging.

RESPOND ONLY WITH THE MODIFICATION RESPONSE:
"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            return {
                'text': response.strip(),
                'type': 'modification'
            }
            
        except Exception as e:
            self.logger.error(f"❌ Failed to generate modification response: {e}")
            return {
                'text': "Bien sûr ! Que souhaitez-vous modifier dans vos préférences ?",
                'type': 'modification'
            }
    
    async def _generate_general_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate general conversation response"""
        try:
            # Check if we need more information
            preference_count = len(context.user_preferences)
            
            if preference_count < 2:
                # Need more preferences
                prompt = f"""
You are ASIA.fr Agent, a friendly and enthusiastic French travel specialist with a warm personality like Layla.ai. You're passionate about helping people discover amazing Asian destinations and you show genuine excitement about their travel dreams.

CURRENT PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}
USER INPUT: {context.user_input}

Generate a warm, friendly response in French that:
• Uses emojis naturally to express enthusiasm and warmth 🌟✨
• Acknowledges their input with genuine interest
• Asks for missing preferences using bullet points (•) with line breaks
• Shows excitement about helping them plan their dream trip
• Uses a conversational, friendly tone like talking to a friend
• Provides helpful examples with enthusiasm

IMPORTANT FORMATTING RULES:
• Use bullet points (•) instead of numbers
• Add line breaks after each bullet point
• Make each bullet point on its own line
• Include relevant emojis to make it more engaging
• Keep it conversational, warm and encouraging

Example format:
• 🌍 Première question avec ligne de retour

• ⏱️ Deuxième question avec ligne de retour

• 💰 Troisième question avec ligne de retour

RESPOND ONLY WITH THE RESPONSE TEXT:
"""
            else:
                # Have enough preferences, ask for confirmation
                prompt = f"""
You are ASIA.fr Agent, a friendly and enthusiastic French travel specialist with a warm personality like Layla.ai. You're excited to help them find their perfect Asian adventure!

CURRENT PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}
USER INPUT: {context.user_input}

Generate a warm, enthusiastic response in French that:
1. 🌟 Summarizes their preferences with excitement
2. ✨ Shows genuine enthusiasm about finding them the perfect offers
3. 🎯 Asks for confirmation to show offers with warmth
4. Uses emojis naturally to express your excitement
5. Sounds like you're genuinely happy to help them

Keep it friendly, warm and encouraging - like talking to a friend!

RESPOND ONLY WITH THE RESPONSE TEXT:
"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            formatted_response = self._format_with_bullet_points(response.strip())
            return {
                'text': formatted_response,
                'type': 'general'
            }
            
        except Exception as e:
            self.logger.error(f"❌ Failed to generate general response: {e}")
            return self._fallback_response()
    
    def _fallback_response(self) -> Dict[str, Any]:
        """Fallback response when generation fails"""
        return {
            'text': "Je suis désolé, j'ai rencontré une difficulté. Pouvez-vous reformuler votre demande ?",
            'type': 'error'
        }
    
    def _format_with_bullet_points(self, text: str) -> str:
        """Format text to ensure proper bullet points with line breaks"""
        import re
        
        # Replace "1. ", "2. ", "3. ", "4. " etc. with "• "
        text = re.sub(r'^\d+\.\s*', '• ', text, flags=re.MULTILINE)
        
        # Split text into lines and process each line
        lines = text.split('\n')
        formatted_lines = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('•'):
                # This is a bullet point line
                # Ensure the bullet point content is properly formatted
                content = line[1:].strip()
                formatted_lines.append(f"• {content}")
                formatted_lines.append("")  # Add empty line after each bullet point
            elif line:
                # Regular text line
                formatted_lines.append(line)
        
        # Join lines and clean up
        result = '\n'.join(formatted_lines)
        
        # Remove multiple consecutive empty lines
        result = re.sub(r'\n{3,}', '\n\n', result)
        
        return result.strip()
    
    async def _generate_preference_summary_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate preference summary response"""
        offers = context.get_metadata('offers', [])
        
        # Create a detailed preference summary text
        summary_text = self._create_preference_summary_text(context.user_preferences)
        
        return {
            'text': f"J'ai bien noté vos préférences de voyage. Voici un récapitulatif de ce que j'ai compris :\n\n{summary_text}\n\nSi ces préférences vous conviennent, je vais vous montrer les meilleures offres. Si vous souhaitez modifier quelque chose, dites-moi simplement ce que vous voulez changer.",
            'type': 'preference_summary',
            'offers': offers,
            'show_preference_summary': True
        }
    
    def _create_preference_summary_text(self, preferences: Dict[str, Any]) -> str:
        """Create preference summary text"""
        parts = []
        
        if preferences.get('destination'):
            parts.append(f"🌍 Destination : {preferences['destination']}")
        if preferences.get('duration'):
            parts.append(f"⏱️ Durée : {preferences['duration']}")
        if preferences.get('budget'):
            parts.append(f"💰 Budget : {preferences['budget']}")
        if preferences.get('style'):
            parts.append(f"🎯 Style : {preferences['style']}")
        if preferences.get('group_size'):
            parts.append(f"👥 Groupe : {preferences['group_size']}")
        if preferences.get('timing'):
            parts.append(f"📅 Période : {preferences['timing']}")
        
        return "\n".join(parts) if parts else "Aucune préférence spécifique détectée" 