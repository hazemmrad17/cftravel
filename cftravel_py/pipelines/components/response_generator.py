"""
Response Generator Component for ASIA.fr Agent
Handles text generation and response formatting
"""

import json
from typing import Dict, Any, List
from ..core import PipelineComponent, PipelineContext, PipelineState
from services.llm_service import LLMService
from services.memory_service import MemoryService

class ResponseGeneratorComponent(PipelineComponent):
    """Generates intelligent responses based on pipeline context"""
    
    def __init__(self, llm_service: LLMService, memory_service: MemoryService):
        super().__init__("ResponseGenerator", priority=70)
        self.llm_service = llm_service
        self.memory_service = memory_service
    
    def is_required(self, context: PipelineContext) -> bool:
        """Always required to generate a response"""
        return True
    
    async def process(self, context: PipelineContext) -> PipelineContext:
        """Generate appropriate response based on context"""
        try:
            # Determine response type and generate accordingly
            response_type = context.get_metadata('response_type', 'question')
            intent = context.get_metadata('intent', 'general')
            
            if intent == 'greeting':
                # Handle greetings, thanks, and goodbyes naturally
                response = await self._generate_natural_response(context, "greeting")
            elif intent == 'preference_complete':
                # Generate text-based preference summary
                response = await self._generate_preference_summary_response(context)
            elif intent == 'confirmation':
                # User confirmed preferences - ensure offers are generated first
                context.add_metadata('should_show_offers', True)
                # The recommendation engine will be called by the pipeline to generate offers
                response = await self._generate_natural_response(context, "confirmation")
            elif intent == 'modification':
                # User wants to modify preferences - help them naturally
                response = await self._generate_natural_response(context, "modification")
            elif intent == 'suggestion_request':
                # User wants AI suggestions - provide helpful recommendations
                response = await self._generate_natural_response(context, "suggestion")
            elif intent == 'general':
                # General conversation - respond naturally
                response = await self._generate_natural_response(context, "conversation")
            elif context.get_metadata('should_show_offers', False):
                # Check if offers are available (generated by recommendation engine)
                offers = context.get_metadata('offers', [])
                if offers:
                    # Generate offer presentation response
                    response = await self._generate_offer_response(context)
                else:
                    # Offers not ready yet, generate confirmation message
                    response = await self._generate_confirmation_response(context)
            else:
                # Generate general conversation response
                response = await self._generate_general_response(context)
            
            # Store response in context
            context.add_metadata('generated_response', response)
            context.add_metadata('response_text', response.get('text', ''))
            
            return context
            
        except Exception as e:
            self.log_error(context, e)
            # Fallback response
            context.add_metadata('generated_response', self._fallback_response())
            return context
    
    async def _generate_offer_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate response when showing offers"""
        offers = context.get_metadata('offers', [])
        
        if not offers:
                    return {
            'text': "Je n'ai pas trouvÃ© d'offres correspondant exactement Ã  vos critÃ¨res. Pouvez-vous prÃ©ciser vos prÃ©fÃ©rences ?",
            'type': 'no_offers',
            'offers': []
        }
    
    async def _generate_natural_response(self, context: PipelineContext, response_type: str) -> Dict[str, Any]:
        """Generate natural, intelligent responses using LLM"""
        try:
            # Build context-aware prompt
            prompt = self._build_natural_response_prompt(context, response_type)
            
            # Get LLM response
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            
            return {
                'text': response.strip(),
                'type': response_type,
                'should_show_offers': response_type == 'confirmation'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Natural response generation failed: {e}")
            # Fallback response
            return {
                'text': "Je suis dÃ©solÃ©, je n'ai pas bien compris. Pouvez-vous reformuler ? ğŸ˜Š",
                'type': response_type,
                'should_show_offers': False
            }
    
    def _build_natural_response_prompt(self, context: PipelineContext, response_type: str) -> str:
        """Build prompt for natural response generation"""
        user_input = context.user_input
        preferences = context.user_preferences
        conversation_history = context.conversation_history[-5:] if context.conversation_history else []
        
        # Build context string
        context_str = ""
        if preferences:
            context_str = f"\nPrÃ©fÃ©rences actuelles: {json.dumps(preferences, indent=2, ensure_ascii=False)}"
        
        history_str = ""
        if conversation_history:
            history_str = f"\nHistorique rÃ©cent: {conversation_history}"
        
        base_prompt = f"""
Tu es ASIA.fr Agent, un spÃ©cialiste du voyage en Asie. Tu dois rÃ©pondre en franÃ§ais de maniÃ¨re naturelle et chaleureuse.

CONTEXTE:{context_str}{history_str}

MESSAGE UTILISATEUR: "{user_input}"

TYPE DE RÃ‰PONSE REQUISE: {response_type}

INSTRUCTIONS:
- RÃ©ponds de maniÃ¨re naturelle et conversationnelle
- Sois chaleureux et enthousiaste
- Utilise des emojis appropriÃ©s
- Adapte ta rÃ©ponse au contexte et Ã  l'historique
- Si c'est une modification, aide l'utilisateur Ã  ajuster ses prÃ©fÃ©rences
- Si c'est une suggestion, propose des idÃ©es crÃ©atives
- Si c'est une confirmation, confirme et prÃ©pare pour les offres
- Si c'est une conversation gÃ©nÃ©rale, sois utile et engageant

RÃ‰PONSE:
"""
        
        # Add specific instructions based on response type
        if response_type == "greeting":
            base_prompt += "\n- Si c'est un remerciement, rÃ©ponds poliment\n- Si c'est un au revoir, sois chaleureux\n- Si c'est une salutation, propose ton aide"
        elif response_type == "modification":
            base_prompt += "\n- Aide l'utilisateur Ã  modifier ses prÃ©fÃ©rences\n- Propose des alternatives\n- Sois flexible et comprÃ©hensif"
        elif response_type == "suggestion":
            base_prompt += "\n- Propose des destinations intÃ©ressantes\n- SuggÃ¨re des expÃ©riences uniques\n- Sois crÃ©atif et inspirant"
        elif response_type == "confirmation":
            base_prompt += "\n- Confirme les prÃ©fÃ©rences\n- PrÃ©pare pour l'affichage des offres\n- Sois enthousiaste"
        elif response_type == "conversation":
            base_prompt += "\n- RÃ©ponds naturellement Ã  la conversation\n- Sois utile et engageant\n- Guide vers la planification de voyage si appropriÃ©"
        
        return base_prompt
    
    async def _generate_offer_intro(self, context: PipelineContext, offers: List[Dict]) -> str:
        """Generate personalized introduction for offers"""
        try:
            prompt = f"""
You are ASIA.fr Agent, a French travel specialist. Generate a personalized introduction for the offers you're about to present.

USER PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}
OFFER COUNT: {len(offers)}

Generate a warm, personalized introduction in French that:
1. Acknowledges their preferences
2. Mentions the number of offers found
3. Invites them to explore the offers
4. Is friendly and professional

Keep it concise (2-3 sentences maximum).

RESPOND ONLY WITH THE INTRODUCTION TEXT:
"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to generate offer intro: {e}")
            return f"Voici {len(offers)} offres qui correspondent parfaitement Ã  vos critÃ¨res :"
    
    async def _generate_confirmation_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate response for user confirmations"""
        try:
            prompt = f"""
You are ASIA.fr Agent, a friendly and enthusiastic French travel specialist with a warm personality like Layla.ai. The user has confirmed their preferences and you're excited to show them amazing offers!

USER PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}

Generate a warm, enthusiastic confirmation response in French that:
1. ğŸŒŸ Acknowledges their confirmation with genuine excitement
2. âœ¨ Shows enthusiasm about preparing their personalized offers
3. Uses emojis naturally to express your happiness
4. Sounds like you're genuinely excited to help them

Keep it warm and encouraging (1-2 sentences).

RESPOND ONLY WITH THE CONFIRMATION TEXT:
"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            return {
                'text': response.strip(),
                'type': 'confirmation'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to generate confirmation response: {e}")
            return {
                'text': "Parfait ! Je prÃ©pare vos offres personnalisÃ©es.",
                'type': 'confirmation'
            }
    
    async def _generate_modification_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate response for modification requests"""
        try:
            prompt = f"""
You are ASIA.fr Agent, a friendly and enthusiastic French travel specialist with a warm personality like Layla.ai. The user wants to modify their preferences and you're happy to help them get it perfect!

CURRENT PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}
USER INPUT: {context.user_input}

Generate a warm, helpful response in French that:
1. ğŸŒŸ Acknowledges their desire to modify preferences with understanding
2. âœ¨ Shows enthusiasm about helping them get their perfect trip
3. Asks what they'd like to change with genuine interest
4. Uses emojis naturally to keep it friendly
5. Sounds like you're genuinely happy to help them adjust

Keep it conversational, warm and encouraging.

RESPOND ONLY WITH THE MODIFICATION RESPONSE:
"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            return {
                'text': response.strip(),
                'type': 'modification'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to generate modification response: {e}")
            return {
                'text': "Bien sÃ»r ! Que souhaitez-vous modifier dans vos prÃ©fÃ©rences ?",
                'type': 'modification'
            }
    
    async def _generate_general_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate general conversation response"""
        try:
            # Check if we need more information
            preference_count = len(context.user_preferences)
            
            if preference_count < 2:
                # Need more preferences
                prompt = f"""
You are ASIA.fr Agent, a friendly and enthusiastic French travel specialist with a warm personality like Layla.ai. You're passionate about helping people discover amazing Asian destinations and you show genuine excitement about their travel dreams.

CURRENT PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}
USER INPUT: {context.user_input}

Generate a warm, friendly response in French that:
â€¢ Uses emojis naturally to express enthusiasm and warmth ğŸŒŸâœ¨
â€¢ Acknowledges their input with genuine interest
â€¢ Asks for missing preferences using bullet points (â€¢) with line breaks
â€¢ Shows excitement about helping them plan their dream trip
â€¢ Uses a conversational, friendly tone like talking to a friend
â€¢ Provides helpful examples with enthusiasm

IMPORTANT FORMATTING RULES:
â€¢ Use bullet points (â€¢) instead of numbers
â€¢ Add line breaks after each bullet point
â€¢ Make each bullet point on its own line
â€¢ Include relevant emojis to make it more engaging
â€¢ Keep it conversational, warm and encouraging

Example format:
â€¢ ğŸŒ PremiÃ¨re question avec ligne de retour

â€¢ â±ï¸ DeuxiÃ¨me question avec ligne de retour

â€¢ ğŸ’° TroisiÃ¨me question avec ligne de retour

RESPOND ONLY WITH THE RESPONSE TEXT:
"""
            else:
                # Have enough preferences, ask for confirmation
                prompt = f"""
You are ASIA.fr Agent, a friendly and enthusiastic French travel specialist with a warm personality like Layla.ai. You're excited to help them find their perfect Asian adventure!

CURRENT PREFERENCES: {json.dumps(context.user_preferences, indent=2, ensure_ascii=False)}
USER INPUT: {context.user_input}

Generate a warm, enthusiastic response in French that:
1. ğŸŒŸ Summarizes their preferences with excitement
2. âœ¨ Shows genuine enthusiasm about finding them the perfect offers
3. ğŸ¯ Asks for confirmation to show offers with warmth
4. Uses emojis naturally to express your excitement
5. Sounds like you're genuinely happy to help them

Keep it friendly, warm and encouraging - like talking to a friend!

RESPOND ONLY WITH THE RESPONSE TEXT:
"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_service.create_generation_completion(messages, stream=False)
            formatted_response = self._format_with_bullet_points(response.strip())
            return {
                'text': formatted_response,
                'type': 'general'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to generate general response: {e}")
            return self._fallback_response()
    
    def _fallback_response(self) -> Dict[str, Any]:
        """Fallback response when generation fails"""
        return {
            'text': "Je suis dÃ©solÃ©, j'ai rencontrÃ© une difficultÃ©. Pouvez-vous reformuler votre demande ?",
            'type': 'error'
        }
    
    def _format_with_bullet_points(self, text: str) -> str:
        """Format text to ensure proper bullet points with line breaks"""
        import re
        
        # Replace "1. ", "2. ", "3. ", "4. " etc. with "â€¢ "
        text = re.sub(r'^\d+\.\s*', 'â€¢ ', text, flags=re.MULTILINE)
        
        # Split text into lines and process each line
        lines = text.split('\n')
        formatted_lines = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('â€¢'):
                # This is a bullet point line
                # Ensure the bullet point content is properly formatted
                content = line[1:].strip()
                formatted_lines.append(f"â€¢ {content}")
                formatted_lines.append("")  # Add empty line after each bullet point
            elif line:
                # Regular text line
                formatted_lines.append(line)
        
        # Join lines and clean up
        result = '\n'.join(formatted_lines)
        
        # Remove multiple consecutive empty lines
        result = re.sub(r'\n{3,}', '\n\n', result)
        
        return result.strip()
    
    async def _generate_preference_summary_response(self, context: PipelineContext) -> Dict[str, Any]:
        """Generate preference summary response"""
        offers = context.get_metadata('offers', [])
        
        # Create a detailed preference summary text
        summary_text = self._create_preference_summary_text(context.user_preferences)
        
        return {
            'text': f"J'ai bien notÃ© vos prÃ©fÃ©rences de voyage. Voici un rÃ©capitulatif de ce que j'ai compris :\n\n{summary_text}\n\nSi ces prÃ©fÃ©rences vous conviennent, je vais vous montrer les meilleures offres. Si vous souhaitez modifier quelque chose, dites-moi simplement ce que vous voulez changer.",
            'type': 'preference_summary',
            'offers': offers,
            'show_preference_summary': True
        }
    
    def _create_preference_summary_text(self, preferences: Dict[str, Any]) -> str:
        """Create preference summary text"""
        parts = []
        
        if preferences.get('destination'):
            parts.append(f"ğŸŒ Destination : {preferences['destination']}")
        if preferences.get('duration'):
            parts.append(f"â±ï¸ DurÃ©e : {preferences['duration']}")
        if preferences.get('budget'):
            parts.append(f"ğŸ’° Budget : {preferences['budget']}")
        if preferences.get('style'):
            parts.append(f"ğŸ¯ Style : {preferences['style']}")
        if preferences.get('group_size'):
            parts.append(f"ğŸ‘¥ Groupe : {preferences['group_size']}")
        if preferences.get('timing'):
            parts.append(f"ğŸ“… PÃ©riode : {preferences['timing']}")
        
        return "\n".join(parts) if parts else "Aucune prÃ©fÃ©rence spÃ©cifique dÃ©tectÃ©e" 